baseado no paper dc3, que está vinculado no chat, verifique as funcoes gt, g_templog, jac_gt e jac_templog para ver se fqaz jus ao paper pois, no meu exemplo, so possuo restricao de desigualdade. se tiver algo a corrigir, corrija pois o meu resultado final não está gerando valores aceitáveis: 
    # def Cost
    def obj_fn(self, y):  # ,d, pumps, tanks, pipes, valves, timeInc):
        # COM EFICIÊNCIA

        log_cost=opt_func.CostOptimizationLog()
        
        with torch.no_grad():
            y_np = y.detach().cpu().numpy()
            
            cost_list = [opt_func.Cost(i, self.d, log_cost, 3) for i in y_np]
            cost_tensor = torch.tensor(cost_list, dtype=torch.float32, device=y.device)

        # Armazenar o custo (para loss), e depois aplicar gradiente manual via backward hook
        cost_tensor.requires_grad_(True)        

        return cost_tensor

    def gT(self, x, y):
        log_tank = opt_func.TanksOptimizationLog()

        #with torch.no_grad():
        y_np = y.detach().cpu().numpy()
        gt_list = [opt_func.gT(i, self.d, 0, log_tank) for i in y_np]
        
        return torch.tensor(gt_list, dtype=torch.float32, device=y.device)

    def g_TempLog(self, x): 
        #with torch.no_grad():
        x_np = x.detach().cpu().numpy()
        g_templog_list = [opt_func.g_TempLog(i, self.d) for i in x_np]
        
        return torch.tensor(g_templog_list, dtype=torch.float32, device=x.device)


    def jac_gT(self, y):
        log_tank = opt_func.TanksOptimizationLog()

        #with torch.no_grad():
        y_np = y.detach().cpu().numpy()
        jac_gt_list = [opt_func.jac_gT(i, self.d, 0, log_tank) for i in y_np]
        
        #jac_gt_list_partial = partial(jac_gt_list)

        jac_list_pos = torch.tensor(jac_gt_list, dtype=torch.float32, device=y.device)
        jac_list_neg = -jac_list_pos.clone()                
        
        return torch.cat((jac_list_pos, jac_list_neg), dim=1)

        
    def jac_TempLog(self, x):
        #with torch.no_grad():
        x_np = x.detach().cpu().numpy()
        jac_templog_list = opt_func.jac_TempLog(x_np, self.d)
        
        return torch.tensor(jac_templog_list, dtype=torch.float32, device=x.device)


    def ineq_dist(self, x, y):

        ineq_dist = self.ineq_resid(x, y)

        return ineq_dist

    # ineq_dist
    def ineq_resid(self, x, y):

        d = self.d
        n_min = d.hmin[0]  # 2
        n_max = d.hmax[0]  # 8

        n_min = 2
        n_max = 7


        gT = self.gT(x, y)

        # Constraint: [gt - Nmax] <= 0 [samples x 10]
        gt_up = gT - n_max

        # Constraint: [Nmin - gT] <= 0 [samples x 10]
        gt_down = n_min - gT

        # [samples x 5]
        g_TempLog = self.g_TempLog(y)

        # [samples x 25]
        return torch.cat([gt_up, gt_down, g_TempLog], dim=1)

    def ineq_jac(self, Y):

        # [samples x 20 x 10]
        jac_gT = self.jac_gT(Y)

        # [5 x 10]
        jac_TempLog = self.jac_TempLog(Y)

        # [samples x 5 x 10]
        jac_combined = torch.cat([jac_gT, jac_TempLog.unsqueeze(0).repeat(jac_gT.shape[0], 1, 1)], dim=1)


        return jac_combined

    def ineq_grad(self, x, y):
        # [samples x 25] ineq_resid = ineq_dist
        ineq_dist_relu = torch.clamp(self.ineq_dist(x, y),0)
        
        # [samples x 1 x 25]
        ineq_dist_expanded = ineq_dist_relu.unsqueeze(1)  

        # [samples X 25 X 10]
        ineq_jac = self.ineq_jac(y)        

        return torch.matmul(ineq_dist_expanded, ineq_jac).squeeze(1)
    
    
        